{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Natural Disaster Detection System.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/avani28/Natural-Disaster-Detection-System-using-Deep-Learning-and-Computer-Vision-/blob/main/Natural_Disaster_Detection_System.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rcQxj2T4f3L"
      },
      "source": [
        "# **Natural Disaster Detection System**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nO0OVFTokLiS"
      },
      "source": [
        "**Load the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOitY8jbK2su",
        "outputId": "99ca0304-986c-4ee0-85e5-2ce770d1fcab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /gdrive\n",
            "/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZG9SfaB4ycF"
      },
      "source": [
        "**Import Necessary Packages**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nAGc1yEfRv1"
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import cv2\n",
        "import sys\n",
        "import os\n",
        "import tempfile\n",
        "from imutils import paths\n",
        "from collections import deque\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.callbacks import *\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwwvpfAI-fcT"
      },
      "source": [
        "***Cyclical Learning Rate Callback***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xypUdSB9x0R"
      },
      "source": [
        "class CyclicLR(Callback):\n",
        "\t\n",
        "\tdef __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n",
        "\t\t\t\t gamma=1., scale_fn=None, scale_mode='cycle'):\n",
        "\t\tsuper(CyclicLR, self).__init__()\n",
        "\n",
        "\t\tself.base_lr = base_lr\n",
        "\t\tself.max_lr = max_lr\n",
        "\t\tself.step_size = step_size\n",
        "\t\tself.mode = mode\n",
        "\t\tself.gamma = gamma\n",
        "\t\tif scale_fn == None:\n",
        "\t\t\tif self.mode == 'triangular':\n",
        "\t\t\t\tself.scale_fn = lambda x: 1.\n",
        "\t\t\t\tself.scale_mode = 'cycle'\n",
        "\t\t\telif self.mode == 'triangular2':\n",
        "\t\t\t\tself.scale_fn = lambda x: 1 / (2. ** (x - 1))\n",
        "\t\t\t\tself.scale_mode = 'cycle'\n",
        "\t\t\telif self.mode == 'exp_range':\n",
        "\t\t\t\tself.scale_fn = lambda x: gamma ** (x)\n",
        "\t\t\t\tself.scale_mode = 'iterations'\n",
        "\t\telse:\n",
        "\t\t\tself.scale_fn = scale_fn\n",
        "\t\t\tself.scale_mode = scale_mode\n",
        "\t\tself.clr_iterations = 0.\n",
        "\t\tself.trn_iterations = 0.\n",
        "\t\tself.history = {}\n",
        "\n",
        "\t\tself._reset()\n",
        "\n",
        "\tdef _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "\t\t\t   new_step_size=None):\n",
        "\t\tif new_base_lr != None:\n",
        "\t\t\tself.base_lr = new_base_lr\n",
        "\t\tif new_max_lr != None:\n",
        "\t\t\tself.max_lr = new_max_lr\n",
        "\t\tif new_step_size != None:\n",
        "\t\t\tself.step_size = new_step_size\n",
        "\t\tself.clr_iterations = 0.\n",
        "\n",
        "\tdef clr(self):\n",
        "\t\tcycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "\t\tx = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "\t\tif self.scale_mode == 'cycle':\n",
        "\t\t\treturn self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "\t\telse:\n",
        "\t\t\treturn self.base_lr + (self.max_lr - self.base_lr) * np.maximum(0, (1 - x)) * self.scale_fn(\n",
        "\t\t\t\tself.clr_iterations)\n",
        "\n",
        "\tdef on_train_begin(self, logs={}):\n",
        "\t\tlogs = logs or {}\n",
        "\n",
        "\t\tif self.clr_iterations == 0:\n",
        "\t\t\tK.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "\t\telse:\n",
        "\t\t\tK.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "\tdef on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "\t\tlogs = logs or {}\n",
        "\t\tself.trn_iterations += 1\n",
        "\t\tself.clr_iterations += 1\n",
        "\n",
        "\t\tself.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n",
        "\t\tself.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "\t\tfor k, v in logs.items():\n",
        "\t\t\tself.history.setdefault(k, []).append(v)\n",
        "\n",
        "\t\tK.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP-fSq3z_Pto"
      },
      "source": [
        "***Learning Rate Finder***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRMVujBG_gD5"
      },
      "source": [
        "class LearningRateFinder:\n",
        "\tdef __init__(self, model, stopFactor=4, beta=0.98):\n",
        "\t\t\n",
        "\t\tself.model = model\n",
        "\t\tself.stopFactor = stopFactor\n",
        "\t\tself.beta = beta\n",
        "\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef reset(self):\n",
        "\t\t\n",
        "\t\tself.lrs = []\n",
        "\t\tself.losses = []\n",
        "\t\tself.lrMult = 1\n",
        "\t\tself.avgLoss = 0\n",
        "\t\tself.bestLoss = 1e9\n",
        "\t\tself.batchNum = 0\n",
        "\t\tself.weightsFile = None\n",
        "\n",
        "\tdef is_data_iter(self, data):\n",
        "\t\t\n",
        "\t\titerClasses = [\"NumpyArrayIterator\", \"DirectoryIterator\",\n",
        "\t\t\t \"DataFrameIterator\", \"Iterator\", \"Sequence\"]\n",
        "\n",
        "\t\t\n",
        "\t\treturn data.__class__.__name__ in iterClasses\n",
        "\n",
        "\tdef on_batch_end(self, batch, logs):\n",
        "\t\tlr = K.get_value(self.model.optimizer.lr)\n",
        "\t\tself.lrs.append(lr)\n",
        "\n",
        "\t\t\n",
        "\t\tl = logs[\"loss\"]\n",
        "\t\tself.batchNum += 1\n",
        "\t\tself.avgLoss = (self.beta * self.avgLoss) + ((1 - self.beta) * l)\n",
        "\t\tsmooth = self.avgLoss / (1 - (self.beta ** self.batchNum))\n",
        "\t\tself.losses.append(smooth)\n",
        "\n",
        "\t\t\n",
        "\t\tstopLoss = self.stopFactor * self.bestLoss\n",
        "\n",
        "\t\t\n",
        "\t\tif self.batchNum > 1 and smooth > stopLoss:\n",
        "\t\t\t\n",
        "\t\t\tself.model.stop_training = True\n",
        "\t\t\treturn\n",
        "\n",
        "\t\t\n",
        "\t\tif self.batchNum == 1 or smooth < self.bestLoss:\n",
        "\t\t\tself.bestLoss = smooth\n",
        "\n",
        "\t\t\n",
        "\t\tlr *= self.lrMult\n",
        "\t\tK.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "\tdef find(self, trainData, startLR, endLR, epochs=None,\n",
        "\t\tstepsPerEpoch=None, batchSize=32, sampleSize=2048,\n",
        "\t\tverbose=1):\n",
        "\t\t\n",
        "\t\tself.reset()\n",
        "\n",
        "\t\t\n",
        "\t\tuseGen = self.is_data_iter(trainData)\n",
        "\n",
        "\t\t\n",
        "\t\tif useGen and stepsPerEpoch is None:\n",
        "\t\t\tmsg = \"Using generator without supplying stepsPerEpoch\"\n",
        "\t\t\traise Exception(msg)\n",
        "\n",
        "\t\telif not useGen:\n",
        "\t\t\tnumSamples = len(trainData[0])\n",
        "\t\t\tstepsPerEpoch = np.ceil(numSamples / float(batchSize))\n",
        "\n",
        "\t\tif epochs is None:\n",
        "\t\t\tepochs = int(np.ceil(sampleSize / float(stepsPerEpoch)))\n",
        "\n",
        "\t\tnumBatchUpdates = epochs * stepsPerEpoch\n",
        "\n",
        "\t\tself.lrMult = (endLR / startLR) ** (1.0 / numBatchUpdates)\n",
        "\n",
        "\t\tself.weightsFile = tempfile.mkstemp()[1]\n",
        "\t\tself.model.save_weights(self.weightsFile)\n",
        "\n",
        "\t\torigLR = K.get_value(self.model.optimizer.lr)\n",
        "\t\tK.set_value(self.model.optimizer.lr, startLR)\n",
        "\n",
        "\t\tcallback = LambdaCallback(on_batch_end=lambda batch, logs:\n",
        "\t\t\tself.on_batch_end(batch, logs))\n",
        "\n",
        "\t\tif useGen:\n",
        "\t\t\tself.model.fit_generator(\n",
        "\t\t\t\ttrainData,\n",
        "\t\t\t\tsteps_per_epoch=stepsPerEpoch,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tverbose=verbose,\n",
        "\t\t\t\tcallbacks=[callback])\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tself.model.fit(\n",
        "\t\t\t\ttrainData[0], trainData[1],\n",
        "\t\t\t\tbatch_size=batchSize,\n",
        "\t\t\t\tepochs=epochs,\n",
        "\t\t\t\tcallbacks=[callback],\n",
        "\t\t\t\tverbose=verbose)\n",
        "\n",
        "\t\tself.model.load_weights(self.weightsFile)\n",
        "\t\tK.set_value(self.model.optimizer.lr, origLR)\n",
        "\n",
        "\tdef plot_loss(self, skipBegin=10, skipEnd=1, title=\"\"):\n",
        "\t\tlrs = self.lrs[skipBegin:-skipEnd]\n",
        "\t\tlosses = self.losses[skipBegin:-skipEnd]\n",
        "\n",
        "\t\tplt.plot(lrs, losses)\n",
        "\t\tplt.xscale(\"log\")\n",
        "\t\tplt.xlabel(\"Learning Rate (Log Scale)\")\n",
        "\t\tplt.ylabel(\"Loss\")\n",
        "\n",
        "\t\tif title != \"\":\n",
        "\t\t\tplt.title(title)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIkOkpUV_tI6"
      },
      "source": [
        "**Set the Path to Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGQgYSvzAUPR"
      },
      "source": [
        "DATASET_PATH = '/gdrive/MyDrive/Cyclone_Wildfire_Flood_Earthquake_Database'\n",
        "Cyclone='/gdrive/MyDrive/Cyclone_Wildfire_Flood_Earthquake_Database/Cyclone'\n",
        "Earthquake='/gdrive/MyDrive/Cyclone_Wildfire_Flood_Earthquake_Database/Earthquake'\n",
        "Wildfire=\"/gdrive/MyDrive/Cyclone_Wildfire_Flood_Earthquake_Database/Flood\"\n",
        "Flood=\"/gdrive/MyDrive/Cyclone_Wildfire_Flood_Earthquake_Database/Wildfire\"\n",
        "\n",
        "# initializing the class labels in dataset\n",
        "CLASSES = [\"Cyclone\", \"Earthquake\", \"Flood\", \"Wildfire\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2B3gTBAiso"
      },
      "source": [
        "**Set the Hyperparameter Values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ta7GfCARAp0M"
      },
      "source": [
        "MIN_LR = 1e-6  #minimum learning rate\n",
        "MAX_LR = 1e-4  #maximum learning rate\n",
        "BATCH_SIZE = 32  #batch size\n",
        "STEP_SIZE = 8  #step size\n",
        "CLR_METHOD = \"triangular\"  #Cyclical Learning Rate Method\n",
        "NUM_EPOCHS = 48  #number of epochs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyhKs9IkBH8N"
      },
      "source": [
        "**Set the Path to Saved Model and Output Curves**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty5K0rM-BXGN"
      },
      "source": [
        "output = '/content/output'\n",
        "MODEL_PATH = os.path.sep.join([\"/content/output\", \"natural_disaster.model\"])\n",
        "\n",
        "LRFIND_PLOT_PATH = os.path.sep.join([\"/content/output\", \"lrfind_plot.png\"])\n",
        "TRAINING_PLOT_LOSS_PATH = os.path.sep.join([\"/content/output\", \"training_plot_loss.png\"])\n",
        "CLR_PLOT_PATH = os.path.sep.join([\"/content/output\", \"clr_plot.png\"])\n",
        "TRAINING_PLOT_ACCURACY_PATH = os.path.sep.join([\"/content/output\", \"training_plot_accuracy.png\"])\n",
        "CONFUSION_MATRIX_PATH =  os.path.sep.join([\"/content/output\", \"confusion_matrix.png\"])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWJIZSr0DMaT"
      },
      "source": [
        "**Data Splitting and Image Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOosbxEBDTsS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551b659e-d067-41d2-f432-426aeab38ec6"
      },
      "source": [
        "#define train,test,validation split ratio\n",
        "TRAIN_SPLIT = 0.75\n",
        "VAL_SPLIT = 0.1\n",
        "TEST_SPLIT = 0.25\n",
        "\n",
        "print(\"Loading images...\")\n",
        "imagePaths = list(paths.list_images(DATASET_PATH))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "for imagePath in imagePaths:\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\n",
        "\timage = cv2.imread(imagePath)  #load the image\n",
        "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  #convert it to RGB channel ordering\n",
        "\timage = cv2.resize(image, (224, 224))  # resize it to be a fixed 224x224 pixels, ignoring aspect ratio\n",
        "\n",
        "\tdata.append(image)\n",
        "\tlabels.append(label)\n",
        "\n",
        "print(\"processing images...\")\n",
        "data = np.array(data, dtype=\"float32\")\n",
        "labels = np.array(labels)\n",
        " \n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "# partition the data into training and testing splits\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels,\n",
        "\ttest_size=TEST_SPLIT, random_state=42)\n",
        "\n",
        "# take the validation split from the training split\n",
        "(trainX, valX, trainY, valY) = train_test_split(trainX, trainY,\n",
        "\ttest_size=VAL_SPLIT, random_state=84)\n",
        "\n",
        "# initialize the training data augmentation object\n",
        "aug = ImageDataGenerator(\n",
        "\trotation_range=30,\n",
        "\tzoom_range=0.15,\n",
        "\twidth_shift_range=0.2,\n",
        "\theight_shift_range=0.2,\n",
        "\tshear_range=0.15,\n",
        "\thorizontal_flip=True,\n",
        "\tfill_mode=\"nearest\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading images...\n",
            "processing images...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRthZNV4E4qW"
      },
      "source": [
        "**Load VGG16 Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyQ6I_EUE_PM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1042dfef-91a9-4e9b-91af-14f5fcb1ce26"
      },
      "source": [
        "baseModel = VGG16(weights=\"imagenet\", include_top=False,\n",
        "\tinput_tensor=Input(shape=(224, 224, 3)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enC1qFeqFHax"
      },
      "source": [
        "**Build the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vnzmbwC83uG"
      },
      "source": [
        "headModel = baseModel.output\n",
        "headModel = Flatten(name=\"flatten\")(headModel)\n",
        "headModel = Dense(512, activation=\"relu\")(headModel)\n",
        "headModel = Dropout(0.5)(headModel)\n",
        "headModel = Dense(len(CLASSES), activation=\"softmax\")(headModel)\n",
        "\n",
        "model = Model(inputs=baseModel.input, outputs=headModel)\n",
        "\n",
        "for layer in baseModel.layers:\n",
        "\tlayer.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dzBPd5UFahU"
      },
      "source": [
        "**Compile Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX0tNq0JFZtV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c840fc0-77ee-4011-e560-75b6d50e0233"
      },
      "source": [
        "print(\"Compiling model...\")\n",
        "opt = SGD(lr=MIN_LR, momentum=0.9)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
        "\tmetrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compiling model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csmdJ5mSFptp"
      },
      "source": [
        "**Find Learning Rate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gp7wWE_BFurA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ed0695f-422f-4028-f301-facd67fd46de"
      },
      "source": [
        "print(\"Finding learning rate...\")\n",
        "lrf = LearningRateFinder(model)\n",
        "lrf.find(\n",
        "\taug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
        "\t\t1e-10, 1e+1,\n",
        "\t\tstepsPerEpoch=np.ceil((trainX.shape[0] / float(BATCH_SIZE))),\n",
        "\t\tepochs=20,\n",
        "\t\tbatchSize=BATCH_SIZE)\n",
        " \n",
        "lrf.plot_loss()\n",
        "plt.savefig(LRFIND_PLOT_PATH)\n",
        " \n",
        "print(\"Learning rate finder complete\")\n",
        "\n",
        "stepSize = STEP_SIZE * (trainX.shape[0] // BATCH_SIZE)\n",
        "clr = CyclicLR(\n",
        "\tmode=CLR_METHOD,\n",
        "\tbase_lr=MIN_LR,\n",
        "\tmax_lr=MAX_LR,\n",
        "\tstep_size=stepSize)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Finding learning rate...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "94/94 [==============================] - 41s 349ms/step - loss: 19.5369 - accuracy: 0.2347\n",
            "Epoch 2/20\n",
            "94/94 [==============================] - 33s 350ms/step - loss: 18.9204 - accuracy: 0.2279\n",
            "Epoch 3/20\n",
            "94/94 [==============================] - 32s 342ms/step - loss: 19.2384 - accuracy: 0.2221\n",
            "Epoch 4/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 19.5485 - accuracy: 0.2276\n",
            "Epoch 5/20\n",
            "94/94 [==============================] - 32s 342ms/step - loss: 19.5789 - accuracy: 0.2152\n",
            "Epoch 6/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 17.5510 - accuracy: 0.2431\n",
            "Epoch 7/20\n",
            "94/94 [==============================] - 32s 344ms/step - loss: 16.4734 - accuracy: 0.2629\n",
            "Epoch 8/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 12.7383 - accuracy: 0.3335\n",
            "Epoch 9/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 8.1487 - accuracy: 0.4867\n",
            "Epoch 10/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 4.1989 - accuracy: 0.6779\n",
            "Epoch 11/20\n",
            "94/94 [==============================] - 32s 340ms/step - loss: 2.9054 - accuracy: 0.7711\n",
            "Epoch 12/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 1.8111 - accuracy: 0.7961\n",
            "Epoch 13/20\n",
            "94/94 [==============================] - 32s 340ms/step - loss: 1.7103 - accuracy: 0.7935\n",
            "Epoch 14/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 3.1463 - accuracy: 0.6733\n",
            "Epoch 15/20\n",
            "94/94 [==============================] - 32s 341ms/step - loss: 5.2949 - accuracy: 0.5004\n",
            "Epoch 16/20\n",
            "94/94 [==============================] - 16s 170ms/step - loss: 9.6215 - accuracy: 0.3957\n",
            "Learning rate finder complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yDeqRU-GC1t"
      },
      "source": [
        "**Train the Network/Fit the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnWJM63kGCba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3df83835-52d1-42a0-b072-3f6e586f8cda"
      },
      "source": [
        "print(\"Training network...\")\n",
        "H = model.fit_generator(\n",
        "\taug.flow(trainX, trainY, batch_size=BATCH_SIZE),\n",
        "\tvalidation_data=(valX, valY),\n",
        "\tsteps_per_epoch=trainX.shape[0] // BATCH_SIZE,\n",
        "\tepochs=NUM_EPOCHS,\n",
        "\tcallbacks=[clr],\n",
        "\tverbose=1)\n",
        "print(\"Network trained\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training network...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/48\n",
            "93/93 [==============================] - 35s 372ms/step - loss: 256.6813 - accuracy: 0.3667 - val_loss: 62.7711 - val_accuracy: 0.6066\n",
            "Epoch 2/48\n",
            "93/93 [==============================] - 33s 355ms/step - loss: 14.7309 - accuracy: 0.7033 - val_loss: 17.0962 - val_accuracy: 0.7808\n",
            "Epoch 3/48\n",
            "93/93 [==============================] - 33s 356ms/step - loss: 5.6595 - accuracy: 0.7886 - val_loss: 6.1854 - val_accuracy: 0.8168\n",
            "Epoch 4/48\n",
            "93/93 [==============================] - 33s 356ms/step - loss: 2.2090 - accuracy: 0.8129 - val_loss: 2.2436 - val_accuracy: 0.8468\n",
            "Epoch 5/48\n",
            "93/93 [==============================] - 33s 358ms/step - loss: 1.4867 - accuracy: 0.8288 - val_loss: 1.5171 - val_accuracy: 0.8589\n",
            "Epoch 6/48\n",
            "93/93 [==============================] - 33s 356ms/step - loss: 1.2211 - accuracy: 0.8396 - val_loss: 1.8892 - val_accuracy: 0.8318\n",
            "Epoch 7/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 1.4116 - accuracy: 0.8484 - val_loss: 1.0082 - val_accuracy: 0.8799\n",
            "Epoch 8/48\n",
            "93/93 [==============================] - 33s 358ms/step - loss: 1.0741 - accuracy: 0.8427 - val_loss: 1.9752 - val_accuracy: 0.8679\n",
            "Epoch 9/48\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 0.9028 - accuracy: 0.8603 - val_loss: 1.2782 - val_accuracy: 0.8889\n",
            "Epoch 10/48\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 0.8224 - accuracy: 0.8610 - val_loss: 1.4184 - val_accuracy: 0.8529\n",
            "Epoch 11/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.5985 - accuracy: 0.8694 - val_loss: 1.0440 - val_accuracy: 0.8829\n",
            "Epoch 12/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.5095 - accuracy: 0.8826 - val_loss: 0.8389 - val_accuracy: 0.8889\n",
            "Epoch 13/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.4346 - accuracy: 0.8823 - val_loss: 0.8915 - val_accuracy: 0.9039\n",
            "Epoch 14/48\n",
            "93/93 [==============================] - 33s 356ms/step - loss: 0.7778 - accuracy: 0.8934 - val_loss: 0.7334 - val_accuracy: 0.8979\n",
            "Epoch 15/48\n",
            "93/93 [==============================] - 33s 358ms/step - loss: 0.6602 - accuracy: 0.8958 - val_loss: 0.8075 - val_accuracy: 0.9069\n",
            "Epoch 16/48\n",
            "93/93 [==============================] - 33s 356ms/step - loss: 0.4646 - accuracy: 0.9009 - val_loss: 0.7180 - val_accuracy: 0.9039\n",
            "Epoch 17/48\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 0.8548 - accuracy: 0.8988 - val_loss: 0.7854 - val_accuracy: 0.9039\n",
            "Epoch 18/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 1.4211 - accuracy: 0.8978 - val_loss: 0.6780 - val_accuracy: 0.9129\n",
            "Epoch 19/48\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 0.3985 - accuracy: 0.8961 - val_loss: 1.0253 - val_accuracy: 0.9189\n",
            "Epoch 20/48\n",
            "93/93 [==============================] - 33s 358ms/step - loss: 0.4459 - accuracy: 0.8914 - val_loss: 0.8082 - val_accuracy: 0.9039\n",
            "Epoch 21/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.4417 - accuracy: 0.8948 - val_loss: 0.7496 - val_accuracy: 0.9159\n",
            "Epoch 22/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.4134 - accuracy: 0.8928 - val_loss: 0.6841 - val_accuracy: 0.9009\n",
            "Epoch 23/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.3736 - accuracy: 0.9012 - val_loss: 0.7001 - val_accuracy: 0.9129\n",
            "Epoch 24/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.3811 - accuracy: 0.8982 - val_loss: 0.8498 - val_accuracy: 0.9189\n",
            "Epoch 25/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.6852 - accuracy: 0.8968 - val_loss: 0.8755 - val_accuracy: 0.8949\n",
            "Epoch 26/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.5742 - accuracy: 0.8948 - val_loss: 1.3519 - val_accuracy: 0.9039\n",
            "Epoch 27/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.4506 - accuracy: 0.9066 - val_loss: 0.6696 - val_accuracy: 0.9099\n",
            "Epoch 28/48\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 0.3495 - accuracy: 0.9026 - val_loss: 0.6015 - val_accuracy: 0.9219\n",
            "Epoch 29/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.5793 - accuracy: 0.9093 - val_loss: 0.7282 - val_accuracy: 0.9159\n",
            "Epoch 30/48\n",
            "93/93 [==============================] - 33s 355ms/step - loss: 0.2796 - accuracy: 0.9141 - val_loss: 0.7882 - val_accuracy: 0.9219\n",
            "Epoch 31/48\n",
            "93/93 [==============================] - 33s 355ms/step - loss: 0.3589 - accuracy: 0.9039 - val_loss: 0.5617 - val_accuracy: 0.9219\n",
            "Epoch 32/48\n",
            "93/93 [==============================] - 33s 355ms/step - loss: 0.2959 - accuracy: 0.9198 - val_loss: 0.6112 - val_accuracy: 0.9249\n",
            "Epoch 33/48\n",
            "93/93 [==============================] - 33s 356ms/step - loss: 0.3134 - accuracy: 0.9144 - val_loss: 0.6066 - val_accuracy: 0.9219\n",
            "Epoch 34/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.2737 - accuracy: 0.9161 - val_loss: 0.5872 - val_accuracy: 0.9189\n",
            "Epoch 35/48\n",
            "93/93 [==============================] - 33s 357ms/step - loss: 0.2431 - accuracy: 0.9229 - val_loss: 0.7727 - val_accuracy: 0.9189\n",
            "Epoch 36/48\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 0.3514 - accuracy: 0.9232 - val_loss: 0.5765 - val_accuracy: 0.9099\n",
            "Epoch 37/48\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 1.1509 - accuracy: 0.9191 - val_loss: 0.6749 - val_accuracy: 0.9159\n",
            "Epoch 38/48\n",
            "93/93 [==============================] - 33s 358ms/step - loss: 0.2783 - accuracy: 0.9168 - val_loss: 0.9445 - val_accuracy: 0.9129\n",
            "Epoch 39/48\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 0.3868 - accuracy: 0.9164 - val_loss: 0.6861 - val_accuracy: 0.8949\n",
            "Epoch 40/48\n",
            "93/93 [==============================] - 33s 358ms/step - loss: 0.4365 - accuracy: 0.9151 - val_loss: 1.2636 - val_accuracy: 0.8979\n",
            "Epoch 41/48\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 0.3985 - accuracy: 0.9097 - val_loss: 1.2365 - val_accuracy: 0.8889\n",
            "Epoch 42/48\n",
            "93/93 [==============================] - 33s 360ms/step - loss: 0.3176 - accuracy: 0.9225 - val_loss: 0.6487 - val_accuracy: 0.9039\n",
            "Epoch 43/48\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 0.2923 - accuracy: 0.9239 - val_loss: 0.7334 - val_accuracy: 0.9009\n",
            "Epoch 44/48\n",
            "93/93 [==============================] - 33s 359ms/step - loss: 0.2734 - accuracy: 0.9141 - val_loss: 0.6471 - val_accuracy: 0.9039\n",
            "Epoch 45/48\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 0.3105 - accuracy: 0.9124 - val_loss: 0.6947 - val_accuracy: 0.9039\n",
            "Epoch 46/48\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 0.2542 - accuracy: 0.9303 - val_loss: 0.7365 - val_accuracy: 0.9099\n",
            "Epoch 47/48\n",
            "93/93 [==============================] - 34s 361ms/step - loss: 0.3262 - accuracy: 0.9283 - val_loss: 0.6813 - val_accuracy: 0.9129\n",
            "Epoch 48/48\n",
            "93/93 [==============================] - 34s 360ms/step - loss: 0.3350 - accuracy: 0.9290 - val_loss: 1.1708 - val_accuracy: 0.9129\n",
            "Network trained\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtvtUP2fHo54"
      },
      "source": [
        "**Evaluate the Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Qxg50kHpbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b448198-2774-4591-9c87-c7cec7a062fd"
      },
      "source": [
        "print(\"Evaluating network...\")\n",
        "predictions = model.predict(testX, batch_size=BATCH_SIZE)\n",
        "print('Classification Report: ')\n",
        "print(classification_report(testY.argmax(axis=1),\n",
        "\tpredictions.argmax(axis=1), target_names=CLASSES))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating network...\n",
            "Classification Report: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Cyclone       0.98      0.97      0.98       244\n",
            "  Earthquake       0.97      0.91      0.94       328\n",
            "       Flood       0.86      0.95      0.90       249\n",
            "    Wildfire       0.96      0.95      0.96       286\n",
            "\n",
            "    accuracy                           0.94      1107\n",
            "   macro avg       0.94      0.95      0.94      1107\n",
            "weighted avg       0.95      0.94      0.94      1107\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG0OC-ecHyho"
      },
      "source": [
        "**Save the Model to Disk**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfjZ3jW9HzEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a57ed6ea-ee42-4588-a784-d9145c5b4658"
      },
      "source": [
        "print(\"Serializing network to '{}'...\".format(MODEL_PATH))\n",
        "model.save(MODEL_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Serializing network to '/content/output/natural_disaster.model'...\n",
            "INFO:tensorflow:Assets written to: /content/output/natural_disaster.model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrA_uRiGIMAs"
      },
      "source": [
        "**Plot and Save Loss Curve** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjeBaMoLILVJ"
      },
      "source": [
        "N = np.arange(0, NUM_EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
        "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.show()\n",
        "plt.savefig(TRAINING_PLOT_LOSS_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFzUDzelIYXQ"
      },
      "source": [
        "**Plot and Save Accuracy Curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YidVTHCgIZZE"
      },
      "source": [
        "N = np.arange(0, NUM_EPOCHS)\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
        "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(TRAINING_PLOT_ACCURACY_PATH)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DghZJtI5Iuny"
      },
      "source": [
        "**Plot and Save Learning Rate History Curve**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8yjfBE6IuLz"
      },
      "source": [
        "N = np.arange(0, len(clr.history[\"lr\"]))\n",
        "plt.figure()\n",
        "plt.plot(N, clr.history[\"lr\"])\n",
        "plt.title(\"Cyclical Learning Rate (CLR)\")\n",
        "plt.xlabel(\"Training Iterations\")\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.savefig(CLR_PLOT_PATH)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0uTAt-87o4K"
      },
      "source": [
        "**Plot Confusion Matrix**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSafSOmBDLRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebb8e082-9832-4b0b-d6ef-d87208528b3b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "def plot_confusion_matrix1(y_true, y_pred, classes,\n",
        "                          normalize=False,\n",
        "                          title=None,\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \n",
        "    if not title:\n",
        "        if normalize:\n",
        "            title = 'Normalized confusion matrix'\n",
        "        else:\n",
        "            title = 'Confusion matrix, without normalization'\n",
        "\n",
        "    \n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    \n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    ax.figure.colorbar(im, ax=ax)\n",
        "    \n",
        "    ax.set(xticks=np.arange(cm.shape[1]),\n",
        "           yticks=np.arange(cm.shape[0]),\n",
        "           \n",
        "           xticklabels=classes, yticklabels=classes,\n",
        "           title=title,\n",
        "           ylabel='True label',\n",
        "           xlabel='Predicted label')\n",
        "\n",
        "    \n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "             rotation_mode=\"anchor\")\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i in range(cm.shape[0]):\n",
        "        for j in range(cm.shape[1]):\n",
        "            ax.text(j, i, format(cm[i, j], fmt),\n",
        "                    ha=\"center\", va=\"center\",\n",
        "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    fig.tight_layout()\n",
        "    return ax\n",
        "\n",
        "\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "y_test = testY.argmax(axis=1)\n",
        "y_pred = predictions.argmax(axis=1)\n",
        "lb = [\"Cyclone\", \"Earthquake\", \"Flood\", \"Wildfire\"] #Thunderstorm, Building_Collapse\n",
        "# Plot normalized confusion matrix\n",
        "plot_confusion_matrix1(y_test, y_pred, classes=lb, normalize=True,\n",
        "                      title='Normalized confusion matrix')\n",
        "\n",
        "plt.savefig(CONFUSION_MATRIX_PATH)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Normalized confusion matrix\n",
            "[[0.97 0.   0.01 0.02]\n",
            " [0.   0.91 0.09 0.  ]\n",
            " [0.   0.03 0.95 0.02]\n",
            " [0.02 0.   0.02 0.95]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiU1Xobl-vN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cf0edb5-1417-4d9b-a013-a9d2752cd02a"
      },
      "source": [
        "!pip install twilio\n",
        "from twilio.rest import Client"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twilio\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/0e/d54630e6daae43dd74d44a94f52d1072b5332c374d699938d7d1db20a54c/twilio-6.50.1.tar.gz (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from twilio) (1.15.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from twilio) (2018.9)\n",
            "Collecting PyJWT>=1.4.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/5f/5cff1c3696e0d574f5741396550c9a308dde40704d17e39e94b89c07d789/PyJWT-2.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from twilio) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->twilio) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->twilio) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->twilio) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->twilio) (2020.12.5)\n",
            "Building wheels for collected packages: twilio\n",
            "  Building wheel for twilio (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for twilio: filename=twilio-6.50.1-py2.py3-none-any.whl size=1208685 sha256=fd5534674045e08d4e5055fb1fb219c9b2867317d1e79a9909b403ed56d0fbfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/10/6c/1b04371d399b059dcea195e00729e096fd959e1e35b0e7c8a2\n",
            "Successfully built twilio\n",
            "Installing collected packages: PyJWT, twilio\n",
            "Successfully installed PyJWT-2.0.0 twilio-6.50.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqeuJTJsJv2g"
      },
      "source": [
        "**Predict the Video**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NSkAUzSGan3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a62c2a5-540f-4f2e-9685-fb1a0bd833f5"
      },
      "source": [
        "from twilio.rest import Client\n",
        "input='/gdrive/MyDrive/videos/cyclone_1.mp4'\n",
        "\n",
        "size=128\n",
        "display=1\n",
        "\n",
        "# load the trained model from disk\n",
        "print(\"Loading model and label binarizer...\")\n",
        "model = load_model(MODEL_PATH)\n",
        "\n",
        "mean = np.array([123.68, 116.779, 103.939][::1], dtype=\"float32\")\n",
        "Q = deque(maxlen=size)  #predictions queue\n",
        "\n",
        "print(\"Processing video...\")\n",
        "vs = cv2.VideoCapture(input)  #initializing video stream\n",
        "writer = None  #pointer to output video file\n",
        "(W, H) = (None, None)  #intialize frame dimensions\n",
        " \n",
        "client = Client(\"ACac0f843371e740077a4aa7734e4e2ad7\", \"4a5605c2a39d5b18dc0383f68b6b7415\")\n",
        "prelabel = ''\n",
        "ok = 'Normal'\n",
        "fi_label = []\n",
        "framecount = 0\n",
        "\n",
        "while True:\n",
        "  (grabbed,frame)  = vs.read()\n",
        "\n",
        "  if not grabbed:\n",
        "    break\n",
        "  if W is None or H is None:\n",
        "    (H,W)=frame.shape[:2]\n",
        "  framecount = framecount + 1\n",
        "\n",
        "  output=frame.copy()\n",
        "  frame=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
        "  frame=cv2.resize(frame,(224,224))\n",
        "  frame=frame.astype(\"float32\")\n",
        "  frame=frame - mean\n",
        "\n",
        "  preds = model.predict(np.expand_dims(frame,axis=0))[0]\n",
        "  prediction=preds.argmax(axis=0)\n",
        "  Q.append(preds)\n",
        "\n",
        "  results = np.array(Q).mean(axis=0)\n",
        "  maxprobab=np.max(results)\n",
        "  i=np.argmax(results)\n",
        "  label=CLASSES[i]\n",
        "\n",
        "  rest = 1-maxprobab\n",
        "\n",
        "  diff= (maxprobab)-(rest)\n",
        "  th=100\n",
        "  if diff>0.80:\n",
        "    th=diff\n",
        "    fi_label = np.append(fi_label, label)\n",
        "    text = \"Alert : {} - {:.2f}%\".format((label), maxprobab * 100)\n",
        "    cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 5)\n",
        "\n",
        "    if label != prelabel:\n",
        "      client.messages \\\n",
        "      .create(to=\"+911234567890\", \n",
        "            from_=\"+19388882407\", \n",
        "            body='\\n'+ str(text))\n",
        "    prelabel = label\n",
        "\n",
        "  if writer is None:\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    writer = cv2.VideoWriter('/content/output/result.mp4', fourcc, 30,(W, H), True)\n",
        "\n",
        "  writer.write(output)\n",
        "\n",
        "print('Frame count', framecount)\n",
        "print('Count label', fi_label)\t\n",
        "#cv2_imshow(output)\n",
        "writer.release()\n",
        "vs.release()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading model and label binarizer...\n",
            "Processing video...\n",
            "Frame count 436\n",
            "Count label ['Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone' 'Cyclone'\n",
            " 'Cyclone' 'Cyclone']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZUczng1rvhzi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}